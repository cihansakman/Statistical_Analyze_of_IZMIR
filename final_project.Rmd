---
title: "Final Project"
author: "Cihan"
date: "14 05 2021"
output:
  html_document: default
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# **R Final Project**

> The scope of this project is the enviromental analyze of Izmir from the three datasets generated by the Izmir Municipality (IBB Acik Veri
Portali https://acikveri.bizizmir.com/dataset). In this project we will try to analyze the Water Consumption and Production and The Amount of Food Distriubted for the Animals.

***

## Imports
```{r}
library(ggplot2)
library('stringr')
library(dplyr)
library(ggpubr)
library(moments)

```

***

### Loading Dataset
```{r}
water.consumption <- read.csv("C:\\Users\\sakma\\Desktop\\R Files\\Final Project\\csv_files\\yillik-mahalle-su-tuketimi.csv", header = TRUE, encoding="UTF-8", stringsAsFactors=FALSE,sep=";")

water.binding <- read.csv("C:\\Users\\sakma\\Desktop\\R Files\\Final Project\\csv_files\\su-baglama-sureleri.csv", header = TRUE, encoding="UTF-8", stringsAsFactors=FALSE, sep=";")

water.production <- read.csv("C:\\Users\\sakma\\Desktop\\R Files\\Final Project\\csv_files\\su-uretimi-kaynak-dagilimi.csv", header = TRUE, encoding="UTF-8", stringsAsFactors=FALSE,sep=";")

```

***

Change the column names
```{r}
colnames(water.consumption) <- c("year","month","district","neighborhood","user_count","avg_consumption")
colnames(water.production) <- c("year","month","source","amount_m3")
colnames(water.binding) <- c("year","district","binding_time_day","sub_time_day","explore_time_day","collection_time_day","petition_count")


```

### **Data visualization and descriptive stats**

> Let's see some of data

>- Summarize all three data
```{r echo=FALSE}
paste("Summary of Water Consumption")
summary(water.consumption)
paste("Summary of Water Production")
summary(water.production)
paste("Summary of Water Binding")
summary(water.binding)

```


>There are some mistakes in the data.

1. In **Water Consumption**, *'year'* and *'month'* columns' type should be String, *'avg_consumpiton'* column kept as String and numbers are seperated by **','**. We'll replace these ',' with '.' and convert the type into **float**.

2. In **Water Production**, *'year'* and *'month'* columns' type should be String, *'amount_m3* should kept as **float**.

3. In **Animal Food**, *'year'* and *'month'* columns' type should be String.

4. For all data we convert *'character'* type variable into *'factor'*

```{r echo=TRUE}

water.consumption$avg_consumption <- str_replace_all(water.consumption$avg_consumption , ',', '.')

paste("Edit for Water Consumption")
water.consumption$avg_consumption = as.numeric(as.character(water.consumption$avg_consumption))
water.consumption$year = as.character(as.numeric(water.consumption$year))
water.consumption$month = as.character(as.numeric(water.consumption$month))

names <- c('year' ,'month', 'district','neighborhood')
water.consumption[,names] <- lapply(water.consumption[,names] , factor)
paste("Summary for Water Production after Edit")
sapply(water.consumption, summary)

##############

paste("Edit for Water Production")
water.production$amount_m3 = as.numeric(as.character(water.production$amount_m3))
water.production$year = as.character(as.numeric(water.production$year))
water.production$month = as.character(as.numeric(water.production$month))

names <- c('year' ,'month', 'source')
water.production[,names] <- lapply(water.production[,names] , factor)
paste("Summary for Water Production after Edit")
sapply(water.production, summary)

#############
paste("Edit for Water Binding")

water.binding$binding_time_day <- str_replace_all(water.binding$binding_time_day , ',', '.')
water.binding$sub_time_day <- str_replace_all(water.binding$sub_time_day , ',', '.')
water.binding$explore_time_day <- str_replace_all(water.binding$explore_time_day , ',', '.')
water.binding$collection_time_day <- str_replace_all(water.binding$collection_time_day , ',', '.')

water.binding$binding_time_day = as.numeric(as.character(water.binding$binding_time_day))
water.binding$sub_time_day = as.numeric(as.character(water.binding$sub_time_day))
water.binding$explore_time_day = as.numeric(as.character(water.binding$explore_time_day))
water.binding$collection_time_day = as.numeric(as.character(water.binding$collection_time_day))

names <- c('year' ,'district')
water.binding[,names] <- lapply(water.binding[,names] , factor)

paste("Summary for Water Binding after Edit")
head(water.binding,20)
sapply(water.binding, summary)

```



### Let's take the Water Consumption of 2020 and show the most 5 consumer districts in Izmir
- Subset the values which are belong the 2020 and summarize the data.
- Create a new feature that refers total consumption rather than average consumption

**Note:** Due to values are huge for total consumption we'll take the (m^3) / (10^3)
```{r message = FALSE, warning=FALSE}
consumption.2020 = subset(water.consumption, subset = water.consumption$year == 2020)
#Created a new feature called consumption which refers total consumption
consumption.2020$consumption_in_month <- paste((consumption.2020$avg_consumption * consumption.2020$user_count) / 10**3 )
consumption.2020$consumption_in_month = as.numeric(as.character(consumption.2020$consumption_in_month))
head(consumption.2020,10)
summary(consumption.2020)
```


- Show the most consumer districts.
```{r  message = FALSE, warning=FALSE}
#We can see the most consumer districts in 2020 in Izmir.
df1 <- consumption.2020 %>% 
  group_by(district) %>% 
  summarize(total_consumption = sum(consumption_in_month))

df1 <- df1 %>%
  arrange(desc(total_consumption))

df1 %>% tbl_df %>% print(n=40)

df1 <- df1 %>% slice_max(total_consumption, n = 5)


```



- Top 5 consumer districts in Izmir
```{r}
p<-ggplot(data=df1, aes(x=reorder(district, -total_consumption), y=total_consumption)) +
  geom_bar(stat="identity", fill="orange")

p <- p + labs(title = "Most 5 Consumer Districts", x='District', y='Total Consumption(m^3/10^3)')
p


```

### Let's take the most two consumer in 2020 and compare them(Buca and Karabağlar)
```{r  message = FALSE, warning=FALSE}
top.two.consumer = subset(consumption.2020, subset = consumption.2020$district == "BUCA" | consumption.2020$district == "KARABAĞLAR" )
summary(top.two.consumer)
## Be sure about taking the Buca and Karabağlar
unique(top.two.consumer[c("district")])
```

- Let's see how many neighborhoods does districts have 
```{r  message = FALSE, warning=FALSE}
p <- top.two.consumer %>% 
     group_by(district) %>% 
     summarise(n=n_distinct(neighborhood)) %>%
     ggplot(., aes(x=district, y=n)) +
            geom_bar(stat='identity',fill="#d896ff")


p + labs(title = "Count of Neighborhoods for Districts", x='District', y='Count of Neighborhoods')
```

**Note:** It seems that Buca has less neighborhoods but the water consuming in Buca higher than Karabağlar.

- Let's see the mean water consumption for each month 


```{r  message = FALSE, warning=FALSE, echo=FALSE}
top.two.consumer$month = as.numeric(as.character(top.two.consumer$month))
summary_data <- tapply(top.two.consumer$consumption_in_month, list(months = top.two.consumer$month,
                                       districts = as.character(top.two.consumer$district)),
                       FUN = mean, na.rm = TRUE)
summary_data

```

**Note:** It seems that there are some mistakes in data for 4th and 5th months.

**Is water consumption correlated with user count of districts?**


- Show the most crowded districts.
```{r  message = FALSE, warning=FALSE}
#We can see the most consumer districts in 2020 in Izmir.
df <- consumption.2020 %>% 
  group_by(district) %>% 
  summarize(total_user = sum(user_count))

df <- df %>%
  arrange(desc(total_user))

df %>% tbl_df %>% print(n=40)

df <- df %>% slice_max(total_user, n = 5)

```

- Take a closer look at most crowded districts and most consumer districts.
```{r}

p1<-ggplot(data=df, aes(x=reorder(district, -total_user), y=total_user)) +
  geom_bar(stat="identity", fill="orange")

p1 <- p1 + labs(title = "Most 5 Crowded Districts", x='District', y='Total User in a Year')



p2<-ggplot(data=df1, aes(x=reorder(district, -total_consumption), y=total_consumption)) +
  geom_bar(stat="identity", fill="orange")

p2 <- p2 + labs(title = "Most 5 Consumer Districts", x='District', y='Total Consumption(m^3/10^3)')

ggarrange(p1, p2, ncol =1, nrow = 2)



```

- Let's see how consumption changes with the number of users.

```{r}

p <- ggplot(data=consumption.2020, aes(x=consumption_in_month, y=user_count))  + geom_point() 
p + labs(title = "Change of Consumpiton With User Count", x='Avg Consumption', y='User Count')
cor(consumption.2020$consumption_in_month,consumption.2020$user_count)
```

**Note:** We can clearly see from the above plot there is a linear correlation between user counts and water consumption. Now let's visualize the data and see the shape of it.

```{r}
base.plot <- ggplot(consumption.2020, aes(x = consumption_in_month)) +
  xlab("consumption_in_month") 


p1 <- base.plot + geom_histogram()
p2 <- base.plot + geom_boxplot()
p3 <- base.plot + geom_density()


ggarrange(p1, p2, p3, ncol =2, nrow = 2, labels = c( "Geom Histogram","Boxplot","Density"))

#It seems that we have a right skewed data. Let's see the Shapiro Test
skewness(consumption.2020$consumption_in_month, na.rm = TRUE)
```



- We'll have log transform for transform right skewed data to normally distributed data.

```{r}
consumption.2020.transform <- transform(consumption.2020, 
            consumption_in_month = log10(consumption_in_month))
            


base.plot <- ggplot(consumption.2020.transform, aes(x = consumption_in_month)) +
  xlab("consumption_in_month") 


p1 <- base.plot + geom_histogram()
p2 <-ggplot(consumption.2020.transform, aes(y = consumption_in_month)) + geom_boxplot()
p3 <- base.plot + geom_density()


ggarrange(p1, p2, p3, ncol =2, nrow = 2, labels = c( "Geom Histogram","Boxplot","Density"))

#It seems that log transformation didn't work for transform the data to normally distributed. Now take look at skewness rate again
skewness(consumption.2020.transform$consumption_in_month, na.rm = TRUE)

```

**From now on we'll assume that our data is normally distributed**

### **Confidence Intervals**

#### Let's take the Water Consumption of 2020 and compare the consumption based on winter and summer

- First we'll categorize the months and just looking at the winter and summer
```{r}
consumption.2020$month = as.numeric(as.character(consumption.2020$month))
consumption.2020$seasons = cut(consumption.2020$month,c(0,2,5,8,11,12))
levels(consumption.2020$seasons) = c("winter","spring","summer","autumn","winter")
#select(consumption.2020, month, seasons)

#subset the winter and summer
winter.vs.summer = subset(consumption.2020, subset = consumption.2020$seasons == "winter" | consumption.2020$seasons == "summer" )
summary(winter.vs.summer)

qplot(x = seasons, y = consumption_in_month,
      geom = "boxplot", data = winter.vs.summer,
      xlab = "Seasons", 
      ylab = "Consumption in m3 / 10^3",
      fill = I("lightblue"))
```

**The boxplot suggest that winter is associated with less water consumption. There are lots of outliers because of our data is right skewed.**


Let's compute a summary table and see it better.

```{r}

aggregate(consumption_in_month ~ seasons, 
          data = winter.vs.summer, 
          FUN = function(x) {c(mean = mean(x), sd = sd(x))})
```

**Now the summary table shows the difference significantly We shouldn't forget that the consumption_in_month represents (m^3^ / 10^3^). It means that we should multiply the values with 1000 to access real m^3^ values of water consumption.(Also these values are transformed) **


```{r}
#Mean for consumption_in_month(m^3 / 1000)
mean(winter.vs.summer$consumption_in_month)
```

**We assume that we don't have an information about the population varience and we'll use the t-score for calculating Confidence Intervals**

Let's take the winter and summer one by one and compare their Confidence Intervals

```{r}
winter = subset(winter.vs.summer, subset = winter.vs.summer$seasons == "winter")
summer = subset(winter.vs.summer, subset = winter.vs.summer$seasons == "summer")


#Confidence Interval of Winter consumption_in_month(m^3 / 1000)
winter.CI <- t.test(winter$consumption_in_month)$conf.int
winter.CI

#Confidence Interval of Summer consumption_in_month(m^3 / 1000)
summer <- t.test(summer$consumption_in_month)$conf.int
summer
```

**Note:** We took the 0.95 Confidence Intervals of water consumption in Summer and Winter. And we can see that the confidence intervals are not overlapping and Water consumption in Summer and Winter are significantly different from each ohters.

Before going for a t-test let's check if the variance of two groups are equal with F-test.
```{r}
res.ftest <- var.test(consumption_in_month ~ seasons, data = winter.vs.summer)
res.ftest
paste("p-value of F-test smaller than alpha(0.05) that mean variances of two sets of data are different. But we'll assume that the variences are equal.")
```

Now let's test our hypothesis using Welch Two Sample t-test

**Ho:** Mu winter = Mu summer and **H1:** Mu winter != Mu summer

```{r}
winter.summer.t.test <- with(winter.vs.summer, t.test(x=consumption_in_month[seasons=="summer"], 
                     y=consumption_in_month[seasons=="winter"]))
winter.summer.t.test

# Calculate difference in means between smoking and nonsmoking groups
winter.summer.diff <- round(((winter.summer.t.test$estimate[1])*1000) - ((winter.summer.t.test$estimate[2])*1000), 1)

# Confidence level as a %
conf.level <- attr(winter.summer.t.test$conf.int, "conf.level") * 100

```

**Results**: Our study finds that water consumptions are on average `r winter.summer.diff`m^3^ higher in the Summer compared to the Winter (t-statistic `r round(winter.summer.t.test$statistic,2)`, p=`r round(winter.summer.t.test$p.value, 3)`, `r conf.level`% CI [`r round(winter.summer.t.test$conf.int,1)`]m^3^ / 10^3^)


**Conclusion:**Our p-value is less than 0.05(alpha) and also our confidence interval doesn't include 0. That's mean Water Consumption in Winter and Summer significantly different from each others. And we can reject that Mu winter is equal to Mu summer(H0)

***

- Now let's look at the mean water consumption of Buca in 2020 is more than 25(25000m3) or not.

Summarize the water consumption.
```{r}
summary(consumption.2020$consumption_in_month)
paste("It seems that mean consumption in month for 2020 is 10.32(10320m3)")


consumption.2020.buca = subset(consumption.2020, subset = consumption.2020$district == "BUCA")

paste("Print the confidence interval for Buca's total consumption")
buca.CI <- t.test(consumption.2020.buca$consumption_in_month)$conf.int
buca.CI

```
It seems that sample means of Buca water consumption is 95% between the 27.25(27250m^3^) and 31.54(31540m^3^).

Let's test the hypothesis and see the result statistically.

- **H0:** Mu <= 25, **H1:** Mu > 25

```{r}

buca.cons.t.test<- t.test(consumption.2020.buca$consumption_in_month, alternative = c("greater"), mu = 25, conf.level = 0.95)
buca.cons.t.test

conf.level <- attr(buca.cons.t.test$conf.int, "conf.level") * 100
```

**Result:**Our study finds that water consumption for Buca in 2020 are on average, (t-statistic `r round(buca.cons.t.test$statistic,2)`, p=`r round(buca.cons.t.test$p.value, 3)`, CI: `r conf.level`% CI [`r round(buca.cons.t.test$conf.int,1)`]m3)


**Conclusion:** p-value is almost zero and it's less than alpha(0.05). That's mean we can reject H0 which says that average mean of water consumption of Buca in 2020 is less or equal than 25000m3. In other words we can conclude that the avg mean of water consumption of Buca in 2020 is greater than 25000m3.

***

From the most 5 crowded districts Buca and Karabağlar are the districts which has most users. Lets test them if the mean user count of  Buca more than Karabağlar or not. First visualize the histogram and check the shape of data.


```{r}

buca.karabag = subset(consumption.2020, subset = consumption.2020$district == "BUCA" | consumption.2020$district == "KARABAĞLAR" )
summary(buca.karabag)

paste("Let's seperate them and see the shapes better")

buca = subset(buca.karabag, subset = buca.karabag$district == "BUCA")
karabag = subset(buca.karabag, subset = buca.karabag$district == "KARABAĞLAR")
paste("Summary for Buca")
summary(buca)
paste("Summary of Karabağ")
summary(karabag)


paste("Draw the histgorams and see the shape of them")


p1 <- ggplot(buca, aes(x = user_count)) +
  xlab("User Count of Karşıyaka")
# Violin plot
p1 <- p1 + geom_histogram(fill="orange")


p2 <- ggplot(karabag, aes(x = user_count)) +
  xlab("User Count of Konak")
# Violin plot
p2 <- p2 + geom_histogram(fill="orange")

ggarrange(p1, p2, ncol =1, nrow = 2, labels = c( "Buca User Count","Karabağ User Count)"))


paste("Check for the outliers")
p1 <- ggplot(buca, aes(y = user_count)) +
  xlab("User Count of Buca")
# Violin plot
p1 <- p1 + geom_boxplot(fill="orange")


p2 <- ggplot(karabag, aes(y = user_count)) +
  xlab("User Count of Karabağlar")
# Violin plot
p2 <- p2 + geom_boxplot(fill="orange")

ggarrange(p1, p2, ncol =1, nrow = 2, labels = c( "Buca User Count","Karabağ User Count)"))



with(buca, shapiro.test(user_count))# p < 0.05
with(karabag, shapiro.test(user_count))# p < 0.05


```

It seems that both data is right skewed. Let's try log transformation and look data again.
```{r}

buca.log <- transform(buca, 
            user_count = log(user_count))

karabag.log <- transform(karabag, 
            user_count = log(user_count))

p1 <- ggplot(buca.log, aes(x = user_count)) +
  xlab("User Count of Buca")
# Violin plot
p1 <- p1 + geom_density(fill="orange")


p2 <- ggplot(karabag.log, aes(x = user_count)) +
  xlab("User Count of Karabaglar")
# Violin plot
p2 <- p2 + geom_density(fill="orange")


p3 <- ggplot(buca.log, aes(sample = user_count)) + ylab("Buca User Counts") +
  stat_qq() +
  stat_qq_line()

p4 <- ggplot(karabag.log, aes(sample = user_count)) + ylab("Karabaglar User Counts") +
  stat_qq() +
  stat_qq_line()


ggarrange(p1, p3, p2, p4, ncol =2, nrow = 2, labels = c( "Transformed Buca User Count","Transformed Buca User Count","Transformed Karabaglar User Count","Transformed Karabaglar User Count"))

with(buca.log, shapiro.test(user_count))# p < 0.05
with(karabag.log, shapiro.test(user_count))# p < 0.05

```

It seems that we failed to normally distribute our data and from now on we'll assume that our data normally distributed. Now take a look at the confidence intervals of Buca's and Karabaglar's user counts.

```{r}
buca.CI <- t.test(buca$user_count)$conf.int
buca.CI

karabag.CI <- t.test(karabag$user_count)$conf.int
karabag.CI

```

It seems that these two districts are significantly different from each other and Buca's mean of user counts more than Karabağlar. Let's test them and see the difference better. Before test it we'll first have F-test to see if their variance are same or not.
```{r}
res.ftest <- var.test(buca$user_count,karabag$user_count)
res.ftest

paste("p-value of F-test smaller than alpha(0.05) that mean variances of two sets of data are different. But we'll assume that the variences are equal.")
```
**H0:** Mu user_count_Buca <= Mu user_count_Karabag, **H1:**Mu user_count_Buca > Mu user_count_Karabag

```{r}

buca.karabag.t.test <- t.test(buca$user_count, karabag$user_count, var.equal = TRUE, alternative = "greater")
buca.karabag.t.test
# Calculate difference in means between smoking and nonsmoking groups
user.count.diff <- round(buca.karabag.t.test$estimate[1] - buca.karabag.t.test$estimate[2], 1)

# Confidence level as a %
conf.level <- attr(buca.karabag.t.test$conf.int, "conf.level") * 100

```

**Results**: Our study finds that user counts are on average `r user.count.diff`person higher in the Buca compared to the Karabağlar (t-statistic `r round(buca.karabag.t.test$statistic,2)`, p=`r round(buca.karabag.t.test$p.value, 3)`, `r conf.level`% CI [`r round(buca.karabag.t.test$conf.int,1)`]person)

**Conclusion**: Our p-value is too close to 0 and smaller than alpha(0.05) therefore we can reject that average user count of Buca less than Karabağlar. In other words, average user count of Buca higher than Karabağlar.


***

# WATER PRODUCTION

***

- Let's see the data again
```{r}
head(water.production,10)
sapply(water.production, summary)

```

**Note:** There are no sufficiant information about the water production of 2021. Therefore we'll dismiss it.
```{r}
water.production = subset(water.production, subset = water.production$year != "2021")
sapply(water.production, summary)

```
- Take a look at the most productive Sources
```{r}
#We can see the most consumer districts in 2020 in Izmir.
df1 <- water.production %>% 
  group_by(source) %>% 
  summarize(total_amount = sum(amount_m3))

df1 <- df1 %>%
  arrange(desc(total_amount))

df1 %>% tbl_df %>% print(n=40)

```


- Top 5 productive sources in Izmir
```{r  message = FALSE, warning=FALSE}
df1 <- df1 %>% slice_max(total_amount, n = 5)
p<-ggplot(data=df1, aes(x=reorder(source, -total_amount), y=total_amount)) +
  geom_bar(stat="identity", fill="orange")

p <- p + labs(title = "Most 5 Productive Sources", x='Source', y='Total Produce(m^3)')
p
```

#### It seems that the most productive source is Tahtali Baraji.

Let's take a look how the water production changes year by yea in Tahtali Baraji.

-First take the Tahtali Baraji's data.

**Note:** amount_m3 divided by 1000 to see the numbers better.
```{r}
water.production.tahtali = subset(water.production, subset = water.production$source == "Tahtalı Barajı")
water.production.tahtali$amount_m3 = water.production.tahtali$amount_m3 / 10**3
sapply(water.production.tahtali, summary)

p<-ggplot(data=water.production.tahtali, aes(x=year, y=amount_m3)) +
  geom_bar(stat="identity", fill="orange")

p



```

Histograms of Water Production in Tahtali
```{r}
par(mfrow = c(2,3)) # Display plots in a single 2 x 2 figure 
plot(water.production.tahtali$amount_m3)
with(water.production.tahtali, hist(amount_m3))
with(water.production.tahtali, boxplot(amount_m3))
plot(water.production.tahtali$amount_m3, water.production.tahtali$year)
qqnorm(water.production.tahtali$amount_m3)
qqline(water.production.tahtali$amount_m3)
plot(density(water.production.tahtali$amount_m3))

#hist(flu$age, right = FALSE, breaks = seq(0,102,2), col = "firebrick", las = 1, xlab = "Age at death (yrs)", ylab = "Frequency", main = "")

```

**Note:** It seems that we have bimodal distribution. It means that there should be different classes. Therefore we'll divide our data into two groups: *Water Consumpiton Before 2015* and *Water Consumption After 2015* and then we'll plot again.

```{r}
water.production.tahtali$year = as.numeric(as.character(water.production.tahtali$year))
water.production.tahtali$seasons = cut(water.production.tahtali$year,c(2008,2014,2020))
levels(water.production.tahtali$seasons) = c("before_2015","after_2015")

#Divide data into two different classes. before.2015 and after.2015
tahtali.before.2015 =subset(water.production.tahtali, subset = water.production.tahtali$seasons == "before_2015")
tahtali.after.2015 =subset(water.production.tahtali, subset = water.production.tahtali$seasons == "after_2015")


par(mfrow= c(1,2))
plot(density(tahtali.before.2015$amount_m3), main = "Before 2015")
plot(density(tahtali.after.2015$amount_m3), main="After 2015")



par(mfrow = c(2,2)) # Display plots in a single 2 x 2 figure 
with(tahtali.before.2015, hist(amount_m3),main = "Water Prod. Tahtali Before 2015")
with(tahtali.before.2015, boxplot(amount_m3),main = "Water Prod. Tahtali Before 2015")
qqnorm(tahtali.before.2015$amount_m3)
qqline(tahtali.before.2015$amount_m3)
plot(density(tahtali.before.2015$amount_m3),main = "Water Prod. Tahtali Before 2015")
shapiro.test(tahtali.before.2015$amount_m3)

par(mfrow = c(2,2)) # Display plots in a single 2 x 2 figure 
with(tahtali.after.2015, hist(amount_m3),main = "Water Prod. Tahtali After 2015")
with(tahtali.after.2015, boxplot(amount_m3),main = "Water Prod. Tahtali After 2015")
qqnorm(tahtali.after.2015$amount_m3)
qqline(tahtali.after.2015$amount_m3)
plot(density(tahtali.after.2015$amount_m3),main = "Water Prod. Tahtali After 2015")
shapiro.test(tahtali.after.2015$amount_m3)
```

**Note:** After that point *Water Production in Tahtali After 2015* seems normally distributed by the above plots and also from the Shapiro Test. Our p value > 0.05 and that's mean we can not reject that data are not significantly different from normal distribution, but *Water Production in Tahtali Before 2015* doesn't look like normally distributed and we'll assume that both of them are normally distributed.


### Let's compare the Confidence Intervals.

- Our confidince coefficient is 0.95 it's mean that our sample means are appear between Confidince Interval by 95%.

- Confidence Interval of Water Production Before 2015 in Tahtali Baraji
```{r}
before.2015.CI <- t.test(tahtali.before.2015$amount_m3)$conf.int
before.2015.CI
```

For the Water Production in Tahtali Baraji before 2015, 95% of the sample means between the 5291.162 and 5717.806. If we transformed the data again that's mean %95 percentage of the sample means between  5291000.162 m^3^  and 5717000.806 m^3^ water.


- Confidence Interval of Water Production After 2015 in Tahtali Baraji.
```{r}
after.2015.CI <- t.test(tahtali.after.2015$amount_m3)$conf.int
after.2015.CI
```

For the Water Production in Tahtali Baraji after 2015, 95% of the sample means between the 7075.987 7557.701. If we transformed the data again that's mean %95 percentage of the sample means between 7075000.987 m^3^ and 7557000.701 m^3^ water.

***

**Note:** Confidence intervals suggest that the Water Production between *Before 2015* and *After 2015* are significantly different from each others.

***

Before going for a t-test let's check if the variance of two groups are equal with F-test.
```{r}
res.ftest <- var.test(amount_m3 ~ seasons, data = water.production.tahtali)
res.ftest
paste("p-value of F-test greater than alpha(0.05) that means there is no significant difference between the variances of the two sets. Therefore we can continue with the t-test")
```


Now let's test our hypothesis using Welch Two Sample t-test

**Ho:** Mu before_2015 = Mu after_2015 and **H1:** Mu before_2015 != Mu after_2015

```{r}
tahtali.t.test <- with(water.production.tahtali, t.test(x=amount_m3[seasons=="after_2015"], 
                     y=amount_m3[seasons=="before_2015"]))

before.after.2015.diff <- round((tahtali.t.test$estimate[1] - tahtali.t.test$estimate[2]), 1)

# Confidence level as a %
conf.level <- attr(tahtali.t.test$conf.int, "conf.level") * 100
```


**Results**: Our study finds that water productions are on average `r before.after.2015.diff`m^3^/10^3^ higher After 2015 compared to the Before 2015 (t-statistic `r round(tahtali.t.test$statistic,2)`, p=`r round(tahtali.t.test$p.value, 3)`, `r conf.level`% CI [`r round(tahtali.t.test$conf.int,1)`]m^3^ / 10^3^)



**Our p-value is less than 0.05(alpha) and also our confidence interval doesn't include 0. That's mean Water Production after 2015 and before 2015 are significantly different from each others. And we can reject that Mu before_2015 is equal to Mu after_2015(H0). Also we can conclude that from the Confidence Intervals, Tahtali Baraji is much more productive after 2015.**

***

Menemen - Çavuşköy Kuyuları and Sarıkız Kuyuları seems to produce almost same. Let's discover if our hypothesis is statistically true or not.

- First visualize the data and check for the normality.
```{r}

production.menemen = subset(water.production, subset = water.production$source == "Menemen - Çavuşköy Kuyuları")
production.sarikiz = subset(water.production, subset = water.production$source == "Sarıkız Kuyuları")

production.menemen$amount_m3 = production.menemen$amount_m3 / 10**3
production.sarikiz$amount_m3 = production.sarikiz$amount_m3 / 10**3

p1 <- ggplot(production.menemen, aes(sample = amount_m3)) + ylab("Production amount(m3/ 1000)") +
  stat_qq() +
  stat_qq_line()

p2 <- ggplot(production.sarikiz, aes(sample = amount_m3)) + ylab("Production amount(m3/ 1000)") +
  stat_qq() +
  stat_qq_line()



ggarrange(p1, p2, ncol =2, nrow = 1, labels = c( "QNorm of Menemen","QNorm of Sarikiz"))


p1 <- ggplot(production.menemen, aes(y = amount_m3))
p1 <- p1 + geom_boxplot(fill=I("lightblue"))

p2 <- ggplot(production.sarikiz, aes(y = amount_m3))
p2 <- p2 + geom_boxplot(fill=I("lightblue"))


ggarrange(p1, p2, ncol =2, nrow = 1, labels = c( "Boxplot of Menemen","Boxplot of Sarikiz"))



shapiro.test(production.menemen$amount_m3)
shapiro.test(production.sarikiz$amount_m3)
```

**Note:** It seems that in Sarikiz data there are lot's of outliers and missing information. We'll just take the positive values for amount_m3 and plot the boxplot again.
```{r}

production.sarikiz = subset(production.sarikiz, subset = production.sarikiz$amount_m3 > 0)
p1 <- ggplot(production.menemen, aes(y = amount_m3))
p1 <- p1 + geom_boxplot(fill=I("lightblue"))

p2 <- ggplot(production.sarikiz, aes(y = amount_m3))
p2 <- p2 + geom_boxplot(fill=I("lightblue")) 


ggarrange(p1, p2, ncol =2, nrow = 1, labels = c( "Boxplot of Menemen","Boxplot of Sarikiz"))

```
There are still outliers for Menemen and Sarikiz. We'll remove the outliers and plot the data again. We'll use the Iner Quartile Range for detecting outliers. And an outlier would be a point below [Q1- (1.5)IQR] or above [Q3+(1.5)IQR].
```{r}
Q.menemen <- quantile(production.menemen$amount_m3, probs=c(.25, .75), na.rm = FALSE)
Q.sarikiz <- quantile(production.sarikiz$amount_m3, probs=c(.25, .75), na.rm = FALSE)

iqr.menemen <- IQR(production.menemen$amount_m3)
iqr.sarikiz <- IQR(production.sarikiz$amount_m3)

low.menemen<- Q.menemen[1]-1.5*iqr.menemen # Lower Range of Menemen

up.sarikiz <-  Q.sarikiz[2]+1.5*iqr.sarikiz # Upper Range of Sarikiz
low.sarikiz<- Q.sarikiz[1]-1.5*iqr.sarikiz # Lower Range of Sarikiz


production.menemen.outlier <- subset(production.menemen, subset = production.menemen$amount_m3 > low.menemen)
production.sarikiz.outlier <- subset(production.sarikiz, subset = production.sarikiz$amount_m3 < up.sarikiz & production.sarikiz$amount_m3 > low.sarikiz)


p1 <- ggplot(production.menemen.outlier, aes(y = amount_m3))
p1 <- p1 + geom_boxplot(fill=I("lightblue"))

p2 <- ggplot(production.sarikiz.outlier, aes(y = amount_m3))
p2 <- p2 + geom_boxplot(fill=I("lightblue")) 


ggarrange(p1, p2, ncol =2, nrow = 1, labels = c( "Boxplot of Menemen","Boxplot of Sarikiz"))


```
We almost remove all the outliers and the above boxplot suggests that Menemen Kuyulari produce less water than Sarikiz Kuyulari. Now we can check for normality test.

From Shapiro Test our p-values for two data is less than alpha(0.05) than we reject that our sets of data normally distributed. Let's try square root transformation.
```{r}
shapiro.test(production.menemen.outlier$amount_m3)
shapiro.test(production.sarikiz.outlier$amount_m3)
```

```{r}
production.menemen.log <- transform(production.menemen.outlier, 
            amount_m3 = log(amount_m3))

production.sarikiz.log <- transform(production.sarikiz.outlier, 
            amount_m3 = log(amount_m3))

shapiro.test(production.menemen.log$amount_m3)
shapiro.test(production.sarikiz.log$amount_m3)

```
As we can see after transformation p-values for Shapiro normality test became almost zero and we reject that our data normally distributed. From now on we'll assume that our data normally distributed.

- We'll assume that we don't have an information about sample standard deviation and assume that there is no significant difference between variances, and we'll use t-test.

-**H0:** Mu consumptionSarikiz = Mu consumptionMenemen and **H1:** Mu consumptionSarikiz != Mu consumptionMenemen
```{r}


sarikiz.menemen.t.test <- t.test(x=production.sarikiz.outlier$amount_m3, 
                     y=production.sarikiz.outlier$amount_m3, var.equal = TRUE)

cons.sarikiz.menemen.diff <- round((sarikiz.menemen.t.test$estimate[1] - sarikiz.menemen.t.test$estimate[2]), 1)

# Confidence level as a %
conf.level <- attr(sarikiz.menemen.t.test$conf.int, "conf.level") * 100
```


**Results**: Our study finds that water productions are on average `r cons.sarikiz.menemen.diff`m^3^ between the Sarıkız Kuyuları and Menemen - Çavuşköy Kuyuları (t-statistic `r round(sarikiz.menemen.t.test$statistic,2)`, p=`r round(sarikiz.menemen.t.test$p.value, 3)`, `r conf.level`% CI [`r round(sarikiz.menemen.t.test$conf.int,1)`]m^3^ / 10^3^)

**Our p-value is 1(greater than alpha) and Confidence interval includes 0. That's mean we can not reject that there is no significant difference in average water production between Sarıkız Kuyuları and Menemen - Çavuşköy Kuyuları. In other words, we can not say the average water production in Sarıkız Kuyuları and Menemen - Çavuşköy Kuyuları are significantly different from each other.** 



***

### ANOVA

- Let's compare the less productive three source and see if the average production of each are equal or not.

- List the production of the sources by descending order.
```{r}
df1 <- water.production %>% 
  group_by(source) %>% 
  summarize(total_amount = sum(amount_m3))

df1 <- df1 %>%
  arrange(desc(total_amount))

df1 %>% tbl_df %>% print(n=15)

```
As we can see from above the less productive three sources are Güzelhisar Barajı, Ödemiş İçme Suyu Arıtma Tesisleri and Buca ve Sarnıç Kuyuları. Now visualize them and see if the data normally distributed or not. Before seperating the groups amount_m3 will be divided by 10^3^ for observing the values better.
```{r}
water.production.1 <- water.production
water.production.1$amount_m3 <- (water.production$amount_m3) / 1000

water.cons.guzelhasir= subset(water.production.1, subset = source == "Güzelhisar Barajı" & amount_m3 > 0)
water.cons.odemis = subset(water.production.1, subset = source == "Ödemiş İçme Suyu Arıtma Tesisleri" & amount_m3 > 0)
water.cons.buca.sarnic = subset(water.production.1, subset = source == "Buca ve Sarnıç Kuyuları" & amount_m3 > 0)

p1 <- ggplot(water.cons.guzelhasir, aes(sample = amount_m3)) + ylab("Water Production") +
  stat_qq() +
  stat_qq_line()

p2 <- ggplot(water.cons.odemis, aes(sample = amount_m3)) + ylab("Water Production") +
  stat_qq() +
  stat_qq_line()

p3 <- ggplot(water.cons.buca.sarnic, aes(sample = amount_m3)) + ylab("Water Production") +
  stat_qq() +
  stat_qq_line()

ggarrange(p1, p2, p3, ncol =3, nrow = 1, labels = c( "Before Transform","Before Transform",
                                                 "Before Transform"))


shapiro.test(water.cons.guzelhasir$amount_m3)
shapiro.test(water.cons.odemis$amount_m3)
shapiro.test(water.cons.buca.sarnic$amount_m3)

```
From above plots and the Shapiro test we can conclude that three set of data are not normally distributed. Let's transform them and check the normality again.
```{r}

water.cons.guzelhasir.log <- transform(water.cons.guzelhasir, 
            amount_m3 = log10(amount_m3))
water.cons.odemis.log <- transform(water.cons.odemis, 
            amount_m3 = log10(amount_m3))
water.cons.buca.sarnic.log <- transform(water.cons.buca.sarnic, 
            amount_m3 = log10(amount_m3))

p1 <- ggplot(water.cons.guzelhasir.log, aes(sample = amount_m3)) + ylab("Güzelhasır Water Production") +
  stat_qq() +
  stat_qq_line()

p2 <- ggplot(water.cons.odemis.log, aes(sample = amount_m3)) + ylab("Ödemiş Water Production") +
  stat_qq() +
  stat_qq_line()

p3 <- ggplot(water.cons.buca.sarnic.log, aes(sample = amount_m3)) + ylab("Buca-Sarnıç Water Production") +
  stat_qq() +
  stat_qq_line()

ggarrange(p1, p2, p3, ncol =3, nrow = 1, labels = c( "After Transform","After Transform",
                                                 "After Transform"))


shapiro.test(water.cons.guzelhasir.log$amount_m3)
shapiro.test(water.cons.odemis.log$amount_m3)
shapiro.test(water.cons.buca.sarnic.log$amount_m3)
```

After log transformassion the data still not normal and from now on we'll assume that the data normally distributed. Now let's compute the One-way ANOVA test.

**H0:** Mu cons.guzelhasir = Mu cons.odemis = Mu cons.buca.sarnic, **H1:** At least one differs.
```{r}
water.production.1 <- subset(water.production.1, subset =  (source == "Güzelhisar Barajı" | source == "Buca ve Sarnıç Kuyuları" 
                                                                     | source == "Ödemiş İçme Suyu Arıtma Tesisleri") & amount_m3 > 0)

water.production.1.log <- transform(water.production.1, 
            amount_m3 = log10(amount_m3))

# Compute the analysis of variance
res.aov <- aov(amount_m3 ~ source, data = water.production.1.log)
# Summary of the analysis
summary(res.aov)
```
**Result:** The p-value obtained from ANOVA is almost 0 and less than alpha(0.05) therefore we reject that the average water production of three sources is the same. In other words, there is strong evidence that at least one differs.

- Let's compute the Tukey Test and see which are differs.
```{r}

TukeyHSD(res.aov)

```
**Result:** The lower and upper bound of:

- Güzelhisar Barajı-Buca ve Sarnıç Kuyuları: (0.172, 0.333)

- Ödemiş İçme Suyu Arıtma Tesisleri-Buca ve Sarnıç Kuyuları: (0.157, 0.353)

- Ödemiş İçme Suyu Arıtma Tesisleri-Güzelhisar Barajı: (-0.094, 0.101)

**Conclusion:** From the above results we can conclude that:

- The interval of Güzelhisar Barajı-Buca ve Sarnıç Kuyuları doesn't contain 0 therefore there is strong evidence that these two differ.

- Ödemiş İçme Suyu Arıtma Tesisleri-Buca ve Sarnıç Kuyuları: doesn't contain 0 therefore there is strong evidence that these two differ.

- Ödemiş İçme Suyu Arıtma Tesisleri-Güzelhisar Barajı: contains 0 therefore there is strong evidence that these two do not differ.



***

# WATER BINDING

***

- Let's see the data again
```{r}
head(water.binding,10)
sapply(water.binding, summary)

```


### Visualize the data

- Let's take a closer look to binding time. We can explore the shape of the data and also check for the outliers
```{r}
#boxplot(birthwt$birthwt.grams~birthwt$hypertension)


qplot(x =year, y = binding_time_day,
      geom = "boxplot", data = water.binding,
      xlab = "Year", 
      ylab = "Binding Times in Days",
      fill = I("lightblue"))
```

We can see the above boxplot explain the binding times for years. It suggest that in 2020 binding requires less time than other years and it's more symetric than others and narrower than others that is mean that standard deviation of 2020 is a bit smaller than others(Binding time doesn't change a lot). Also in 2019 it seems that the median is also same with in 2020 but the boxplot is wider that is mean the binding time changes much.
 
- Also plot the violin plot and density functions to see the difference better.
```{r}
base.plot <- ggplot(water.binding, aes(x = year, y = binding_time_day)) +
  xlab("Year") +
  ylab("Binding Times in (day)")
# Violin plot
p1 <- base.plot + geom_violin()



#Density functions
base.plot <- ggplot(water.binding, aes(x = binding_time_day)) +
  xlab("Binding Time (day)")
p2 <- base.plot + geom_density(aes(fill = year), alpha = 0.5)

ggarrange(p1, p2, ncol =1, nrow = 2, labels = c( "Violing Histogram","Density"))

```

As we can see from above plots our three data are right skewed but the it suggest us that the 2020 has the less binding time.

- Let's discover the correlation between binding time and exploring time.
```{r}


p <- ggplot(data=water.binding, aes(x=binding_time_day, y=explore_time_day))  + geom_point() 
p + labs(title = "Change of Subscriber Request With Water Binding Time", x='Binding Time in Days', y='Subscriber Request ')


ggscatter(water.binding, x = "binding_time_day", y = "explore_time_day", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Bindin Time (day)", ylab = "Explore Time")


```

From above plots the Pearson coefficient correlation is 0.62. We can not say there is a high correlation but also can not say there is no correlation between them.

Let's viusalize the histograms and try to see the shape of Binding Time and Exploring Time

```{r}

p1 <- ggplot(water.binding, aes(y = binding_time_day)) 
# Violin plot
p1 <- p1 + geom_boxplot(fill=I("lightblue"))


p2 <- ggplot(water.binding, aes(y = explore_time_day))
# Violin plot
p2 <- p2 + geom_boxplot(fill=I("lightblue"))

ggarrange(p1, p2, ncol =1, nrow = 2, labels = c( "Binding Time (day)","Exploring Time (day)"))


p1 <- ggplot(water.binding, aes(x = binding_time_day)) +
  xlab("Binding Times in (day)")
# Violin plot
p1 <- p1 + geom_histogram(fill="orange")


p2 <- ggplot(water.binding, aes(x = explore_time_day)) +
  xlab("Exploring Times in (day)")
# Violin plot
p2 <- p2 + geom_histogram(fill="orange")

ggarrange(p1, p2, ncol =1, nrow = 2, labels = c( "Binding Time (day)","Exploring Time (day)"))




```


It seems that both *Binding Time* and *Exploring Time* are right-skewed. Let's try to transform them with log transform then visualize them again
```{r}
water.binding.log <- transform(water.binding, 
            binding_time_day = log(binding_time_day),
            explore_time_day = log(explore_time_day)
            
            )

p1 <- ggplot(water.binding, aes(x = binding_time_day)) +
  xlab("Binding Time in (day)")
# Violin plot
p1 <- p1 + geom_density(fill="orange")


p2 <- ggplot(water.binding.log, aes(x = binding_time_day)) +
  xlab("Binding Times in (day)")
# Violin plot
p2 <- p2 + geom_density(fill="orange")

p3 <- ggplot(water.binding, aes(sample = binding_time_day)) + ylab("Binding Times in (day)") +
  stat_qq() +
  stat_qq_line()

p4 <- ggplot(water.binding.log, aes(sample = binding_time_day)) + ylab("Binding Times in (day)") +
  stat_qq() +
  stat_qq_line()

ggarrange(p1, p2, p3, p4, ncol =2, nrow = 2, labels = c( "Before Transform","After Transform",
                                                 "Before Transform","After Transform"))

shapiro.test(water.binding.log$binding_time_day)




p1 <- ggplot(water.binding, aes(x = explore_time_day)) +
  xlab("Explore Time in (day)")
# Violin plot
p1 <- p1 + geom_density(fill="orange")


p2 <- ggplot(water.binding.log, aes(x = explore_time_day)) +
  xlab("Explore Times in (day)")
# Violin plot
p2 <- p2 + geom_density(fill="orange")

p3 <- ggplot(water.binding, aes(sample = explore_time_day)) + ylab("Explore Times in (day)") +
  stat_qq() +
  stat_qq_line()

p4 <- ggplot(water.binding.log, aes(sample = explore_time_day)) + ylab("Explore Times in (day)") +
  stat_qq() +
  stat_qq_line()

ggarrange(p1, p2, p3, p4, ncol =2, nrow = 2, labels = c( "Before Transform","After Transform",
                                                 "Before Transform","After Transform"))
shapiro.test(water.binding.log$explore_time_day)

```

It seems that after transformation *Explore Time* value became more normally distributed and *Binding Time* seems still skewed but we'll assume that both data are normally distributed.


- Let's take a look at subscriber petition in last three years
```{r}
p1 <- ggplot(water.binding, aes(sample = petition_count)) + ylab("Request") +
  stat_qq() +
  stat_qq_line()

p2 <- ggplot(water.binding, aes(y = petition_count)) +
  xlab("Request")
# Violin plot
p2 <- p2 + geom_boxplot(fill=I("lightblue"))

p3 <- ggplot(water.binding, aes(x = petition_count)) +
  xlab("Request")
# Violin plot
p3 <- p3 + geom_density(fill=I("lightblue"))

ggarrange(p1, p2, p3, ncol =3, nrow = 1, labels = c( "Before Transform","Before Transform",
                                                 "Before Transform"))


petition.log <- transform(water.binding, 
            petition_count = log(petition_count))


p1 <- ggplot(petition.log, aes(sample = petition_count)) + ylab("Request") +
  stat_qq() +
  stat_qq_line()

p2 <- ggplot(petition.log, aes(y = petition_count)) +
  xlab("Request")
# Violin plot
p2 <- p2 + geom_boxplot(fill=I("lightblue"))

p3 <- ggplot(petition.log, aes(x = petition_count)) +
  xlab("Request")
# Violin plot
p3 <- p3 + geom_density(fill=I("lightblue"))

ggarrange(p1, p2, p3, ncol =3, nrow = 1, labels = c( "After Transform","After Transform",
                                                 "After Transform"))
shapiro.test(petition.log$petition_count)
```

Before the transformation our data was right skewed and we try to take log transformation and make it normally distributed but it seems that doesn't work. After that point we'll assume that our data is normally distributed.

- Let's compare the confidence interval of subscriber petition in 2019 and 2020
```{r}

petition.log.2019 <- subset(petition.log, subset = year == "2019")
petition.log.2020 <- subset(petition.log, subset = year == "2020")



petition.log.2019 <- subset(petition.log, subset = year == "2019")


#Confidence Interval of petition of 2019
petition.log.2019.CI <- t.test(petition.log.2019$petition_count)$conf.int
petition.log.2019.CI

#Confidence Interval of petition of 2020
petition.log.2020.CI <- t.test(petition.log.2020$petition_count)$conf.int
petition.log.2020.CI
```

It seems that there is significant difference between petitions in 2019 and 2020. Lets test it and see it better.

- We'll assume that we don't have an information about sample standard deviation then we'll use t-test.

- Mu petition2019 = Mu petition2020 and H1: Mu petition2019 != Mu petition2020
```{r}


with(petition.log, t.test(x=petition_count[year=="2019"], 
                     y=petition_count[year=="2020"]))


```

**According to the Welch Two Sample t-test, our p-value is 0.6425, and also the 95 percent confidence interval contains 0. That is means we can not reject that the true difference in means is equal to 0. In other words, we can not say the petition counts in 2020 and 2019 are significantly different from each other.**

***

- **Is The tax collection time in 2018 takes more time than 2020? Let's test it.**

- First visualize the data and see the shape of it.
```{r}


collection2020.2018 = subset(water.binding, subset = year != "2019")



#Density functions
base.plot <- ggplot(collection2020.2018, aes(x = collection_time_day)) +
  xlab("Collection Time (day)")
p <- base.plot + geom_density(aes(fill = year), alpha = 0.5)

p

shapiro.test(collection2020.2018$collection_time_day)
```

It seems that our both data right skewed let's try log transformation and make them normally distributed.
```{r}

collection2020.2018.log <- transform(collection2020.2018, 
            collection_time_day = log10(collection_time_day))


#Density functions
base.plot <- ggplot(collection2020.2018.log, aes(x = collection_time_day)) +
  xlab("Collection Time (day)")
p <- base.plot + geom_density(aes(fill = year), alpha = 0.5)

p

qplot(x =year, y = collection_time_day,
      geom = "boxplot", data = collection2020.2018.log,
      xlab = "Year", 
      ylab = "Binding Times in Days",
      fill = I("lightblue"))

shapiro.test(collection2020.2018.log$collection_time_day)
```
Now it looks better and also box plot suggest that tax collection time in 2020 takes less time than 2018. Now separate the data by year and apply the Shapiro Normality test.
```{r}


collection2020.log = subset(collection2020.2018.log, subset = year == "2020")
collection2018.log = subset(collection2020.2018.log, subset = year == "2018")

shapiro.test(collection2020.log$collection_time_day)
shapiro.test(collection2018.log$collection_time_day)
```

Shapiro test also shows that our two sets are normally distributed We can have the two-sample t-test now.

- We'll assume that we don't have an information about sample standard deviation and assume that there is no significant difference between variances, and we'll use t-test.

-**H0:** Mu collectiontime2018 <= Mu collectiontime2020 and **H1:** Mu collectiontime2018 > Mu collectiontime2020
```{r}
collection.time.2020.2018.t.test <- t.test(collection2018.log$collection_time_day, collection2020.log$collection_time_day, var.equal = TRUE, alternative = "greater")
collection.time.2020.2018.t.test
collection.time.2020.2018.diff <- round((collection.time.2020.2018.t.test$estimate[1] - collection.time.2020.2018.t.test$estimate[2]), 1)

# Confidence level as a %
conf.level <- attr(collection.time.2020.2018.t.test$conf.int, "conf.level") * 100

mean.of.transformed.data <- mean(collection2020.2018.log$collection_time_day)
mean.of.transformed.data

```


**Results**: Our study finds that tax collection time are on average in 2018 takes `r ((collection.time.2020.2018.diff)**mean.of.transformed.data)*24`hours more than 2020 (t-statistic `r round(collection.time.2020.2018.t.test$statistic,2)`, p=`r round(collection.time.2020.2018.t.test$p.value, 3)`, `r conf.level`% CI [`r round(collection.time.2020.2018.t.test$conf.int,1)`]

**The p-value is almost zero and less than alpha(0.05). Therefore we can reject the H0 which refers collectiontime2018 <= Mu collectiontime2020. In other words we reject that tax collection time in 2018 takes less time than 2020. We statistically show that tax collection time is shorter in 2020.** 

***


- **Does subscriber's subscription time longer than 30 days in average?**
- First summarize the subscription time of subscribers.
```{r}
summary(water.binding$sub_time_day)
```
It seems that mean is greater than 30 days. Now let's look at the shape of data.
```{r}


p1 <- ggplot(water.binding, aes(sample = sub_time_day)) + ylab("Sub Time(day)") +
  stat_qq() +
  stat_qq_line()

p2 <- ggplot(water.binding, aes(y = sub_time_day)) +
  xlab("Sub Time(day)")
p2 <- p2 + geom_boxplot(fill=I("orange"))

p3 <- ggplot(water.binding, aes(x = sub_time_day)) +
  xlab("Sub Time(day)")

p3 <- p3 + geom_density(fill=I("orange"))

ggarrange(p1, p2, p3, ncol =3, nrow = 1, labels = c( "Before Transform","Before Transform",
                                                 "Before Transform"))


shapiro.test(water.binding$sub_time_day)

```

As we can see from plots and also in Shapiro we reject that our data is normally distributed. Let's try log transformation.
```{r}

sub.time.log <- transform(water.binding, 
            sub_time_day = log10(sub_time_day))

p1 <- ggplot(sub.time.log, aes(sample = sub_time_day)) + ylab("Sub Time(day)") +
  stat_qq() +
  stat_qq_line()

p2 <- ggplot(sub.time.log, aes(y = sub_time_day)) +
  xlab("Sub Time(day)")
p2 <- p2 + geom_boxplot(fill=I("orange"))

p3 <- ggplot(sub.time.log, aes(x = sub_time_day)) +
  xlab("Sub Time(day)")

p3 <- p3 + geom_density(fill=I("orange"))

ggarrange(p1, p2, p3, ncol =3, nrow = 1, labels = c( "After Transform","After Transform",
                                                 "After Transform"))


shapiro.test(sub.time.log$sub_time_day)
```

It seems that after transformed the data it perfectly normally distributed. 

- We'll assume that we don't have an information about sample standard deviation and assume that there is no significant difference between variances, and we'll use t-test.

-**H0:** Mu sub_time_day <= 30 and **H1:** Mu sub_time_day > 30
```{r}
sub.time.t.test<- t.test(sub.time.log$sub_time_day, alternative = c("greater"), mu = log10(30), conf.level = 0.95)
sub.time.t.test

# Confidence level as a %
conf.level <- attr(collection.time.2020.2018.t.test$conf.int, "conf.level") * 100

mean.of.transformed.data <- mean(sub.time.log$sub_time_day)
round(sub.time.t.test$p.value,3)
```


**Results**: Our study finds that (t-statistic `r round(sub.time.t.test$statistic,2)`, p=`r round(sub.time.t.test$p.value, 3)`, `r conf.level`% CI [`r round(sub.time.t.test$conf.int,1)`], mean of data is found `r round(10**mean.of.transformed.data)` days.



**Conclusion:**The p-value is 0.024 and it's less than alpha(0.05). Therefore we reject the H0 which refers Mu sub_time_day <= 30 days. In other words we reject that subscriber's subscription time less than 30 days on average We statistically show that subscriber's subscription time is longer than 30 days.


***

### ANOVA TEST

We were visuilazed and talk about the water binding times of 2020, 2019 and 2018. Let's visualize the boxplots again and test them if the average binding times of these three years are equal or not.
```{r}
#boxplot(birthwt$birthwt.grams~birthwt$hypertension)


qplot(x =year, y = binding_time_day,
      geom = "boxplot", data = water.binding,
      xlab = "Year", 
      ylab = "Binding Times in Days",
      fill = I("lightblue"))
```

Let's summarize the water binding times(day) for years and check for Shapiro Normality Test.

```{r}

group_by(water.binding, year) %>%
  summarise(
    count = n(),
    mean = mean(binding_time_day, na.rm = TRUE),
    sd = sd(binding_time_day, na.rm = TRUE)
  )


shapiro.test(water.binding$binding_time_day)

```
It seems that there is some differences between the average binding times of years. Also the p-value of Shapiro Test less than alpha(0.05) then we reject that the data normally distributed.

Let's take the log of data and try to transform it and check the normality for each year.
```{r}

water.binding.log <- transform(water.binding, 
            binding_time_day = log10(binding_time_day))


water.binding.2020= subset(water.binding.log, subset = year == "2020")
water.binding.2019 = subset(water.binding.log, subset = year == "2019")
water.binding.2018 = subset(water.binding.log, subset = year == "2018")

shapiro.test(water.binding.2020$binding_time_day)
shapiro.test(water.binding.2019$binding_time_day)
shapiro.test(water.binding.2018$binding_time_day)


#Density functions
p <- ggplot(water.binding.log, aes(x = binding_time_day)) +
  xlab("Binding Time (day)")
p <- p + geom_density(aes(fill = year), alpha = 0.5)

p
```

After transformation each year looks like normally distributed. We can conclude them from the Shapiro test. All the p-values are greater than alpha(0.05) then we can not reject that data normally distributed. Now we can compute the One-way ANOVA test.

**H0:** Mu bind_time_2020 =  Mu bind_time_2019 = Mu bind_time_2018, **H1:** At least one differs.
```{r}

# Compute the analysis of variance
res.aov <- aov(binding_time_day ~ year, data = water.binding.log)
summary(res.aov)
```

**Conclusion:** From the above table, we can see the p-value of the ANOVA test is 0.258 which greater than alpha(0.05). Then, we can not reject H0 which refers to three years of average binding time(day) are equal. In other words, there is no strong evidence at least one differs.















